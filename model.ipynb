{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING STEPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&gt; The difference in average earnings between m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The myth is that the \"gap\" is entirely based o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The assertion is that women get paid less for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You said in the OP that's not what they're mea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&gt;Men and women are not payed less for the same...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  label\n",
       "0  > The difference in average earnings between m...      0\n",
       "1  The myth is that the \"gap\" is entirely based o...      0\n",
       "2  The assertion is that women get paid less for ...      0\n",
       "3  You said in the OP that's not what they're mea...      0\n",
       "4  >Men and women are not payed less for the same...      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('traindata.csv')\n",
    "dropped = ['processed', 'offensiveness_score']\n",
    "rename = {'txt': 'comment', 'isOffensive': 'label'}\n",
    "dataset = dataset.drop(columns=dropped)\n",
    "dataset = dataset.rename(columns=rename)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8cbe85b518548b0b4eddaa210a4ccd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AAYUSH\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\AAYUSH\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df1bf8626b84f9fbbaffc755ad9c0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c2d85bf3f44a34912b2e28d45c2315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\AAYUSH\\Desktop\\ToxicityCheck\\ToxicityCheck\\model.ipynb Cell 7\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m padded_token_ids\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Preprocess the train set\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m train_data[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_data[\u001b[39m'\u001b[39;49m\u001b[39mcomment\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(preprocess_comments)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Preprocess the test set\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m test_data[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m test_data[\u001b[39m'\u001b[39m\u001b[39mcomment\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(preprocess_comments)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1104\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1155\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1156\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1157\u001b[0m             values,\n\u001b[0;32m   1158\u001b[0m             f,\n\u001b[0;32m   1159\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1160\u001b[0m         )\n\u001b[0;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1163\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\AAYUSH\\Desktop\\ToxicityCheck\\ToxicityCheck\\model.ipynb Cell 7\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_comments\u001b[39m(comment):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# Tokenize the comment\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mtokenize(comment)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Add the [CLS] and [SEP] tokens\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     tokens \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m[CLS]\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m tokens \u001b[39m+\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m[SEP]\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils.py:547\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m         tokenized_text\u001b[39m.\u001b[39mappend(token)\n\u001b[0;32m    546\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 547\u001b[0m         tokenized_text\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenize(token))\n\u001b[0;32m    548\u001b[0m \u001b[39m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[39mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:249\u001b[0m, in \u001b[0;36mBertTokenizer._tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    247\u001b[0m             split_tokens\u001b[39m.\u001b[39mappend(token)\n\u001b[0;32m    248\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 249\u001b[0m             split_tokens \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwordpiece_tokenizer\u001b[39m.\u001b[39;49mtokenize(token)\n\u001b[0;32m    250\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m     split_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwordpiece_tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:555\u001b[0m, in \u001b[0;36mWordpieceTokenizer.tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    553\u001b[0m end \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(chars)\n\u001b[0;32m    554\u001b[0m cur_substr \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 555\u001b[0m \u001b[39mwhile\u001b[39;00m start \u001b[39m<\u001b[39;49m end:\n\u001b[0;32m    556\u001b[0m     substr \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(chars[start:end])\n\u001b[0;32m    557\u001b[0m     \u001b[39mif\u001b[39;00m start \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def preprocess_comments(comment):\n",
    "    # Tokenize the comment\n",
    "    tokens = tokenizer.tokenize(comment)\n",
    "\n",
    "    # Add the [CLS] and [SEP] tokens\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "\n",
    "    # Convert tokens to token IDs\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # Pad or truncate the token IDs to a fixed length\n",
    "    max_length = 128\n",
    "    padded_token_ids = token_ids[:max_length] + [0] * (max_length - len(token_ids[:max_length]))\n",
    "\n",
    "    return padded_token_ids\n",
    "\n",
    "# Preprocess the train set\n",
    "train_data['input_ids'] = train_data['comment'].apply(preprocess_comments)\n",
    "\n",
    "# Preprocess the test set\n",
    "test_data['input_ids'] = test_data['comment'].apply(preprocess_comments)\n",
    "\n",
    "# Convert the labels to numeric values (0 for non-toxic, 1 for toxic)\n",
    "train_data['label'] = train_data['label'].apply(lambda x: 1 if x == 'toxic' else 0)\n",
    "test_data['label'] = test_data['label'].apply(lambda x: 1 if x == 'toxic' else 0)\n",
    "\n",
    "# Convert the preprocessed data into torch tensors\n",
    "train_input_ids = torch.tensor(train_data['input_ids'].tolist())\n",
    "train_labels = torch.tensor(train_data['label'].tolist())\n",
    "\n",
    "test_input_ids = torch.tensor(test_data['input_ids'].tolist())\n",
    "test_labels = torch.tensor(test_data['label'].tolist())\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = torch.utils.data.TensorDataset(train_input_ids, train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_input_ids, test_labels)\n",
    "\n",
    "# Define batch size and create data loaders\n",
    "batch_size = 32\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT EMBEDDINGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first : <class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"This is the first : {type(train_data_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(batch_input_ids):\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_input_ids)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "    return embeddings\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING ADVERSARIAL TRAINING - DISCRIMINATOR AND VAE MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, hidden_size, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_size * 2)  # Output mean and log-variance\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mean + eps * std\n",
    "        return z\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = x.view(-1, self.hidden_size)\n",
    "        hidden = self.encoder(x)\n",
    "        mean, logvar = hidden[:, :self.latent_size], hidden[:, self.latent_size:]\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        return z, mean, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mean, logvar = self.encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mean, logvar\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Define the hyperparameters for the discriminator\n",
    "input_size = 128\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "\n",
    "# Create an instance of the discriminator\n",
    "discriminator = Discriminator(input_size, hidden_size, output_size)\n",
    "vae = VAE(hidden_size, latent_size=32)\n",
    "# Define the loss function for the discriminator\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "# Define the optimizer for the discriminator\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "vae_optimizer = optim.Adam(vae.parameters(), lr=0.001)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUILDING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturbation_function(toxic_embeddings, epsilon=0.1):\n",
    "    # Generate random noise\n",
    "    noise = torch.randn_like(toxic_embeddings)\n",
    "\n",
    "    # Scale the noise by epsilon\n",
    "    perturbation = epsilon * noise\n",
    "\n",
    "    # Add perturbation to the toxic embeddings\n",
    "    adversarial_embeddings = toxic_embeddings + perturbation\n",
    "\n",
    "    return adversarial_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input tensor([[  101,  1000,  4957,  ...,     0,     0,     0],\n        [  101,  2515,  3087,  ...,     0,     0,     0],\n        [  101,  1000,  2011,  ...,     0,     0,     0],\n        ...,\n        [  101,  1059,  3619,  ...,     0,     0,     0],\n        [  101, 15391,  2045,  ...,     0,     0,     0],\n        [  101,  1000,  1027,  ...,  1998,  3828,  1996]]) is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\AAYUSH\\Desktop\\ToxicityCheck\\ToxicityCheck\\model.ipynb Cell 14\u001b[0m in \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m vae\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_input_ids, batch_labels \u001b[39min\u001b[39;00m train_data_loader:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# Generate BERT embeddings for toxic comments\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     toxic_embeddings \u001b[39m=\u001b[39m generate_embeddings(batch_input_ids)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# Generate adversarial examples by perturbing the toxic embeddings\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     adversarial_embeddings \u001b[39m=\u001b[39m toxic_embeddings \u001b[39m+\u001b[39m perturbation_function(toxic_embeddings)\n",
      "\u001b[1;32mc:\\Users\\AAYUSH\\Desktop\\ToxicityCheck\\ToxicityCheck\\model.ipynb Cell 14\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_embeddings\u001b[39m(comment):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode(comment, add_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     tokens_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([tokens])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AAYUSH/Desktop/ToxicityCheck/ToxicityCheck/model.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model \u001b[39m=\u001b[39m BertModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2332\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[39m@add_end_docstrings\u001b[39m(\n\u001b[0;32m   2296\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[0;32m   2297\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2315\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2316\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:\n\u001b[0;32m   2317\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2318\u001b[0m \u001b[39m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[0;32m   2319\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2330\u001b[0m \u001b[39m            method).\u001b[39;00m\n\u001b[0;32m   2331\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2332\u001b[0m     encoded_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[0;32m   2333\u001b[0m         text,\n\u001b[0;32m   2334\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[0;32m   2335\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m   2336\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[0;32m   2337\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[0;32m   2338\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m   2339\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m   2340\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m   2341\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2342\u001b[0m     )\n\u001b[0;32m   2344\u001b[0m     \u001b[39mreturn\u001b[39;00m encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2740\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2730\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2731\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2732\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2733\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2737\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2738\u001b[0m )\n\u001b[1;32m-> 2740\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[0;32m   2741\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[0;32m   2742\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[0;32m   2743\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m   2744\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[0;32m   2745\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[0;32m   2746\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m   2747\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m   2748\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m   2749\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m   2750\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m   2751\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m   2752\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m   2753\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m   2754\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m   2755\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[0;32m   2756\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m   2757\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   2758\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2759\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m    641\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    642\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 649\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[0;32m    650\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_for_model(\n\u001b[0;32m    653\u001b[0m     first_ids,\n\u001b[0;32m    654\u001b[0m     pair_ids\u001b[39m=\u001b[39msecond_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    668\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m    669\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils.py:635\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    631\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m is not valid. Should be a string or a list/tuple of strings when\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    632\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m `is_split_into_words=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    633\u001b[0m     )\n\u001b[0;32m    634\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 635\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    636\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    637\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m integers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    638\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Input tensor([[  101,  1000,  4957,  ...,     0,     0,     0],\n        [  101,  2515,  3087,  ...,     0,     0,     0],\n        [  101,  1000,  2011,  ...,     0,     0,     0],\n        ...,\n        [  101,  1059,  3619,  ...,     0,     0,     0],\n        [  101, 15391,  2045,  ...,     0,     0,     0],\n        [  101,  1000,  1027,  ...,  1998,  3828,  1996]]) is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    discriminator.train()\n",
    "    vae.train()\n",
    "\n",
    "    for batch_input_ids, batch_labels in train_data_loader:\n",
    "        # Generate BERT embeddings for toxic comments\n",
    "        toxic_embeddings = generate_embeddings(batch_input_ids)\n",
    "\n",
    "        # Generate adversarial examples by perturbing the toxic embeddings\n",
    "        adversarial_embeddings = toxic_embeddings + perturbation_function(toxic_embeddings)\n",
    "\n",
    "        # Train the discriminator\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        real_labels = torch.ones(batch_size, 1)\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "        real_predictions = discriminator(toxic_embeddings)\n",
    "        fake_predictions = discriminator(adversarial_embeddings)\n",
    "        discriminator_loss = adversarial_loss(real_predictions, real_labels) + adversarial_loss(fake_predictions, fake_labels)\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        # Train the VAE\n",
    "        vae_optimizer.zero_grad()\n",
    "        reconstructed_embeddings = vae(toxic_embeddings)\n",
    "        reconstruction_loss = mse_loss(toxic_embeddings, reconstructed_embeddings)\n",
    "        reconstruction_loss.backward()\n",
    "        vae_optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    discriminator.eval()\n",
    "    vae.eval()\n",
    "    # ... perform evaluation on validation set ...\n",
    "\n",
    "# Generate non-toxic comments using the trained VAE\n",
    "non_toxic_rephrase = generate_non_toxic_rephrases(toxic_comment)\n",
    "\n",
    "# Define functions for perturbing toxic embeddings and calculating losses\n",
    "# ... implement perturbation function, adversarial loss, and reconstruction loss functions ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
