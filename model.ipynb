{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING STEPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&gt; The difference in average earnings between m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The myth is that the \"gap\" is entirely based o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The assertion is that women get paid less for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You said in the OP that's not what they're mea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&gt;Men and women are not payed less for the same...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  label\n",
       "0  > The difference in average earnings between m...      0\n",
       "1  The myth is that the \"gap\" is entirely based o...      0\n",
       "2  The assertion is that women get paid less for ...      0\n",
       "3  You said in the OP that's not what they're mea...      0\n",
       "4  >Men and women are not payed less for the same...      0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('traindata.csv')\n",
    "dropped = ['processed', 'offensiveness_score']\n",
    "rename = {'txt': 'comment', 'isOffensive': 'label'}\n",
    "dataset = dataset.drop(columns=dropped)\n",
    "dataset = dataset.rename(columns=rename)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_comments(comment):\n",
    "    # Tokenize the comment\n",
    "    tokens = tokenizer.tokenize(comment)\n",
    "\n",
    "    # Add the [CLS] and [SEP] tokens\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "\n",
    "    # Convert tokens to token IDs\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # Pad or truncate the token IDs to a fixed length\n",
    "    max_length = 128\n",
    "    padded_token_ids = token_ids[:max_length] + [0] * (max_length - len(token_ids[:max_length]))\n",
    "\n",
    "    return padded_token_ids\n",
    "\n",
    "# Preprocess the train set\n",
    "train_data['input_ids'] = train_data['comment'].apply(preprocess_comments)\n",
    "\n",
    "# Preprocess the test set\n",
    "test_data['input_ids'] = test_data['comment'].apply(preprocess_comments)\n",
    "\n",
    "# Convert the labels to numeric values (0 for non-toxic, 1 for toxic)\n",
    "train_data['label'] = train_data['label'].apply(lambda x: 1 if x == 'toxic' else 0)\n",
    "test_data['label'] = test_data['label'].apply(lambda x: 1 if x == 'toxic' else 0)\n",
    "\n",
    "# Convert the preprocessed data into torch tensors\n",
    "train_input_ids = torch.tensor(train_data['input_ids'].tolist())\n",
    "train_labels = torch.tensor(train_data['label'].tolist())\n",
    "\n",
    "test_input_ids = torch.tensor(test_data['input_ids'].tolist())\n",
    "test_labels = torch.tensor(test_data['label'].tolist())\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = torch.utils.data.TensorDataset(train_input_ids, train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_input_ids, test_labels)\n",
    "\n",
    "# Define batch size and create data loaders\n",
    "batch_size = 32\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT EMBEDDINGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(comment):\n",
    "    tokens = tokenizer.encode(comment, add_special_tokens=True)\n",
    "    tokens_tensor = torch.tensor([tokens])\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "    return embeddings.squeeze(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING ADVERSARIAL TRAINING - DISCRIMINATOR AND VAE MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, hidden_size, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_size * 2)  # Output mean and log-variance\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mean + eps * std\n",
    "        return z\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = x.view(-1, self.hidden_size)\n",
    "        hidden = self.encoder(x)\n",
    "        mean, logvar = hidden[:, :self.latent_size], hidden[:, self.latent_size:]\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        return z, mean, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mean, logvar = self.encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mean, logvar\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Define the hyperparameters for the discriminator\n",
    "input_size = 128\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "\n",
    "# Create an instance of the discriminator\n",
    "discriminator = Discriminator(input_size, hidden_size, output_size)\n",
    "vae = VAE(hidden_size, latent_size=32)\n",
    "# Define the loss function for the discriminator\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "# Define the optimizer for the discriminator\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "vae_optimizer = optim.Adam(vae.parameters(), lr=0.001)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUILDING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
